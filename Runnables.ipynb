{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-groq\n",
        "!pip install faiss-cpu\n",
        "!pip install langchain-community\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "import json\n",
        "\n",
        "# Initialize Groq LLM\n",
        "llm = ChatGroq(\n",
        "    model_name=\"deepseek-r1-distill-llama-70b\",\n",
        "    temperature=0.7\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJqaL-1ieZc8",
        "outputId": "a9872374-f449-4cbb-f55d-b8b5afdcf40d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-groq in /usr/local/lib/python3.11/dist-packages (0.2.5)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.42 in /usr/local/lib/python3.11/dist-packages (from langchain-groq) (0.3.45)\n",
            "Requirement already satisfied: groq<1,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from langchain-groq) (0.19.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.12.2)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain-groq) (0.3.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain-groq) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain-groq) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain-groq) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.42->langchain-groq) (24.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.42->langchain-groq) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.42->langchain-groq) (3.10.15)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.42->langchain-groq) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.42->langchain-groq) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.42->langchain-groq) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.42->langchain-groq) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.42->langchain-groq) (2.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "1MHiFzhXoDk0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGroq(\n",
        "    model_name=\"llama-3.3-70b-versatile\",\n",
        "    temperature=0.7\n",
        ") #instanciamos el modelo\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a curious fact about {soccer_player}\") #utilizamos el prompt template\n",
        "\n",
        "output_parser = StrOutputParser() #instanciamos el parses\n",
        "\n",
        "chain = prompt | llm | output_parser #Realizamos la chain\n",
        "\n",
        "chain.invoke({\"soccer_player\": \"Ronaldo\"}) #hacemos el invoke dandole los valores que necesita el prompt templete"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "xiDMRyqUoIzB",
        "outputId": "e6975388-feae-48fa-debc-94e5a6818bff"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'One curious fact about Cristiano Ronaldo is that he has a museum dedicated to himself in his hometown of Funchal, Madeira, Portugal. The museum, called the \"CR7 Museum,\" features a vast collection of his trophies, medals, and memorabilia, including his Ballon d\\'Or awards, UEFA Champions League medals, and even his first football boots. The museum was opened in 2013 and has become a popular tourist destination, with thousands of visitors each year. It\\'s a testament to Ronaldo\\'s incredible career and his dedication to preserving his legacy for his fans.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for s in chain.stream({\"soccer_player\": \"Ronaldo\"}): #esta es la manera de que la respuesta vaya apareciendo de a poco, es una menera de simular una conversacion humana\n",
        "    if hasattr(s, 'content'):\n",
        "        print(s.content, end=\"\", flush=True)\n",
        "    else:\n",
        "        print(s, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SnltVhRpq-L",
        "outputId": "684c44e3-3987-4c34-e229-84c039d70425"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a curious fact about Cristiano Ronaldo: \n",
            "\n",
            "Cristiano Ronaldo is known for his intense training regimen and dedication to fitness. However, what's less well-known is that he has a cryotherapy chamber in his home. Cryotherapy involves exposure to extremely low temperatures (often below -100°C) to aid in muscle recovery and reduce inflammation. Ronaldo uses this chamber to help his body recover from intense matches and training sessions, which is just one example of the extreme measures he takes to maintain his physical fitness and stay at the top of his game."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LCEL Chains/Runnables are used with:\n",
        " * chain.**invoke**(): call the chain on an input.\n",
        "\n",
        "    Recibimos la repuesta del llm en el instante\n",
        " * chain.**stream**(): call the chain on an input and stream back chunks of the response.\n",
        "\n",
        "    Recibimos la respuesta del llm con una cierta pausa de manera tal que paresca una converzacion con un humano\n",
        "\n",
        " * chain.**batch**(): call the chain on a list of inputs.\n",
        "\n",
        "    Podemos realizar la llamada en parallelo"
      ],
      "metadata": {
        "id": "76UnMgNqrv5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Runnables\n",
        "son elementos que pueden ser utilizados en una chain\n",
        "\n",
        "RunnableParallel: correo los procesos en parallelo\n",
        "\n",
        "RunnableLambda: hay que darlñe una entrada de parametros que se los pasar a una funcion lamda y el output sera la salida de la funcion\n",
        "\n",
        "RunnablePassthrough: toma la entrada y la devuelve de manera identica, lo que entra sale"
      ],
      "metadata": {
        "id": "NjbLRM23Mv74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnableLambda, RunnablePassthrough\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Definir la plantilla de prompt\n",
        "prompt = ChatPromptTemplate.from_template(\"Summarize the sensor data: {sensor_id}\")\n",
        "\n",
        "# Definir el parser de salida\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Función para calcular el promedio de las lecturas del sensor\n",
        "def calculate_average(sensor_data):\n",
        "    readings = sensor_data[\"readings\"]\n",
        "    average = sum(readings) / len(readings)\n",
        "    sensor_data[\"average\"] = average\n",
        "    print(sensor_data)\n",
        "    return sensor_data\n",
        "\n",
        "# Función para encontrar la lectura máxima del sensor\n",
        "def find_max_reading(sensor_data):\n",
        "    readings = sensor_data[\"readings\"]\n",
        "    max_reading = max(readings)\n",
        "    sensor_data[\"max_reading\"] = max_reading\n",
        "    print(sensor_data)\n",
        "    return sensor_data\n",
        "\n",
        "# Función para encontrar la lectura mínima del sensor\n",
        "def find_min_reading(sensor_data):\n",
        "    readings = sensor_data[\"readings\"]\n",
        "    min_reading = min(readings)\n",
        "    sensor_data[\"min_reading\"] = min_reading\n",
        "    print(sensor_data)\n",
        "    return sensor_data\n",
        "\n",
        "# Definir la cadena de ejecución con tres runnables en paralelo\n",
        "chain = RunnableParallel(\n",
        "    #calculamos el average, la lectura maxima y minima\n",
        "    {\n",
        "        \"average\": RunnableLambda(calculate_average),\n",
        "        \"max_reading\": RunnableLambda(find_max_reading),\n",
        "        \"min_reading\": RunnableLambda(find_min_reading),\n",
        "        \"sensor_id\": RunnablePassthrough(id)\n",
        "    }\n",
        ") | prompt | llm | output_parser #El runnableparallel procesa tres runnableLambda y un runnanlePassthrough en paralelo por lo cual tendremos cuatro salidas que seran la entrada al promp\n",
        "\n",
        "# Invocar la cadena con datos de entrada\n",
        "result = chain.invoke({\n",
        "    \"id\": \"Sensor_01\",\n",
        "    \"readings\": [23.4, 25.1, 22.8, 24.5, 23.9]\n",
        "})\n",
        "\n",
        "# Imprimir los resultados\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErvNqMccDLUp",
        "outputId": "6e9e8f52-376a-4770-f4ee-a4f9cdeaf298"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'Sensor_01', 'readings': [23.4, 25.1, 22.8, 24.5, 23.9], 'average': 23.939999999999998}\n",
            "{'id': 'Sensor_01', 'readings': [23.4, 25.1, 22.8, 24.5, 23.9], 'average': 23.939999999999998, 'max_reading': 25.1}\n",
            "{'id': 'Sensor_01', 'readings': [23.4, 25.1, 22.8, 24.5, 23.9], 'average': 23.939999999999998, 'max_reading': 25.1, 'min_reading': 22.8}\n",
            "Here's a summary of the sensor data:\n",
            "\n",
            "**Sensor ID:** Sensor_01\n",
            "**Readings:** 5 data points (23.4, 25.1, 22.8, 24.5, 23.9)\n",
            "**Average Reading:** 23.94\n",
            "**Maximum Reading:** 25.1\n",
            "**Minimum Reading:** 22.8\n",
            "\n",
            "Let me know if you'd like me to help with anything else.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS #base de datos vectorial\n",
        "from langchain_core.output_parsers import StrOutputParser #el parser de datos\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "#falta la APIKEY de openIA\n",
        "\n",
        "vectorstore = FAISS.from_texts(\n",
        "    [\"AI Accelera has trained more than 10,000 Alumni from all continents and top companies\"], embedding=OpenAIEmbeddings()\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "retrieval_chain = (\n",
        "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "retrieval_chain.invoke(\"who are the Alumni of AI Accelera?\")"
      ],
      "metadata": {
        "id": "t-dVLpSznKOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from operator import itemgetter\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings #falta la APIkey de openIA\n",
        "\n",
        "vectorstore = FAISS.from_texts(\n",
        "    [\"AI Accelera has trained more than 5,000 Enterprise Alumni.\"], embedding=OpenAIEmbeddings()\n",
        ")\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer in the following language: {language}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = ( #los itemgettet permiten que los runnables tomen un solo item (una sola entrada)\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | retriever,\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "        \"language\": itemgetter(\"language\"),\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain.invoke({\"question\": \"How many Enterprise Alumni has trained AI Accelera?\", \"language\": \"Pirate English\"})"
      ],
      "metadata": {
        "id": "dP7kouPSnsVM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}